{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGMENT ANYTHING MODEL (META)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/facebook/sam-vit-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../..')))\n",
    "\n",
    "from config.path import get_training_data_path\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "import cv2\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTALLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "# %pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P ./weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(os.getcwd(), \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    #points_per_side=20,\n",
    "    pred_iou_thresh=0.96,\n",
    "    #stability_score_thresh=0.98,\n",
    "    #crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'tshirt_1000.png'\n",
    "\n",
    "img_path = get_training_data_path(file_name)\n",
    "image_bgr = cv2.imread(img_path)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "masks = mask_generator.generate(image_rgb)\n",
    "len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming masks is a list of masks\n",
    "masks_greater_than_20000 = [m for m in masks if m[\"area\"] > 20000]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(masks_greater_than_20000), ncols=1, figsize=(12, 14))\n",
    "\n",
    "\n",
    "for i, m in enumerate(masks_greater_than_20000):\n",
    "    mask = np.array(m['segmentation'], dtype=int)\n",
    "    axes[i].imshow(mask)\n",
    "    axes[i].set_title(f'Mask {i+1} + {masks[i][\"stability_score\"]:.2f} + {masks_greater_than_20000[i][\"area\"]}')  # Set title for each subplot\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "\n",
    "    # Sort annotations by area in descending order\n",
    "    #sorted_anns = sorted(anns, key=lambda x: x['area'], reverse=True)\n",
    "    sorted_anns =  [m for m in anns if m[\"area\"] > 20000 and m[\"area\"] < 55000]\n",
    "\n",
    "    # Setup the plot\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    # Create an image array: Initialize with transparency\n",
    "    # img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    # img[:, :, 3] = 0  # Set alpha channel to 0 (fully transparent)\n",
    "\n",
    "    for ann in sorted_anns:\n",
    "        # m = ann['segmentation']\n",
    "        # Random color with fixed alpha\n",
    "        # color_mask = np.concatenate([np.random.random(3), [0.46]])\n",
    "        # img[m] = color_mask\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        # Bbox format: [x_min, y_min, width, height]\n",
    "        rect = patches.Rectangle((ann['bbox'][0], ann['bbox'][1]), ann['bbox'][2], ann['bbox'][3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image_rgb)\n",
    "show_anns(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Assuming 'image' is your original image and 'annotations' is a list of dictionaries containing annotations\n",
    "for i, ann in enumerate(masks_greater_than_20000):\n",
    "    # Extract bounding box coordinates\n",
    "    x, y, width, height = ann['bbox']\n",
    "    \n",
    "    # Define rectangular region\n",
    "    rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    \n",
    "    mask = np.array(ann['segmentation'], dtype=np.uint8)\n",
    "    masked_image = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)\n",
    "    \n",
    "    # Crop the region from the original image\n",
    "    cropped_image = masked_image[y:y+height, x:x+width]\n",
    "\n",
    "    # Save the cropped image\n",
    "    output_path = f\"cropped_image_{i}.jpg\"\n",
    "    cv2.imwrite(output_path, cropped_image)\n",
    "\n",
    "    # Optionally, visualize the cropped region\n",
    "    plt.imshow(cropped_image)\n",
    "    plt.title(f\"Cropped Image {i}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
